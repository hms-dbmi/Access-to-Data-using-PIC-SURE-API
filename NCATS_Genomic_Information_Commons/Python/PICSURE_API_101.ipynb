{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PIC-SURE API tutorial using the GIC institute database\n",
    "This is a tutorial notebook, aimed to be quickly up and running with the Python PIC-SURE API. It covers the main functionalities of the API.\n",
    "\n",
    "## Python PIC-SURE API\n",
    "### What is PIC-SURE?\n",
    "Databases exposed through PIC-SURE API encompass a wide heterogeneity of architectures and data organizations underneath. PIC-SURE hides this complexity and expose the different databases in the same format, allowing researchers to focus on the analysis and medical insights, thus easing the process of reproducible sciences.\n",
    "\n",
    "### More about PIC-SURE\n",
    "PIC-SURE stands for Patient-centered Information Commons: Standardized Unification of Research Elements. The API is available in two different programming languages, Python and R, allowing investigators to query databases in the same way using any of those languages.\n",
    "\n",
    "PIC-SURE is a large project from which the R/Python PIC-SURE API is only a brick. Among other things, PIC-SURE also offers a graphical user interface, allowing research scientist to get quick knowledge about variables and data available for a specific data source.\n",
    "\n",
    "The API is actively developed by the Avillach-Lab at Harvard Medical School.\n",
    "\n",
    "GitHub repo:\n",
    "\n",
    "* https://github.com/hms-dbmi/pic-sure-python-adapter-hpds\n",
    "* https://github.com/hms-dbmi/pic-sure-python-client\n",
    "\n",
    "---\n",
    "\n",
    "## Getting your own user-specific security token\n",
    "**Before running this notebook, please be sure to review the get_your_token.ipynb notebook. It contains explanation about how to get a security token, mandatory to access the databases.**\n",
    "\n",
    "### Environment set-up\n",
    "\n",
    "#### Pre-requisites: \n",
    "* Python >= 3.7\n",
    "* pip: Python package manager, already available in most system with a Python interpreter installed ([pip installation instructions])(https://pip.pypa.io/en/stable/installing/).\n",
    "\n",
    "#### IPython magic command\n",
    "Those two lines of code below do load the autoreload IPython extension. Although not necessary to execute the rest of the Notebook, it does enable to reload every dependency each time python code is executed, thus enabling to take into account changes in external file imported into this Notebook (e.g. user defined function stored in separate file), without having to manually reload libraries. Turns out very handy when developing interactively. More about [IPython Magic commands].(https://ipython.readthedocs.io/en/stable/interactive/magics.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Packages installation\n",
    "Using the pip package manager, we install the packages listed in the `requirements.txt` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up environment\n",
    "import sys\n",
    "!{sys.executable} -m pip install -r requirements.txt\n",
    "!{sys.executable} -m pip install --upgrade --force-reinstall git+https://github.com/hms-dbmi/pic-sure-python-adapter-hpds.git\n",
    "!{sys.executable} -m pip install --upgrade --force-reinstall git+https://github.com/hms-dbmi/pic-sure-python-client.git\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports\n",
    "Import all the external dependencies, as well as user-defined functions stored in the _python_lib_ folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful to estimate execution time of the Notebook\n",
    "from datetime import datetime\n",
    "then = datetime.now()\n",
    "\n",
    "# pic-sure api lib\n",
    "import PicSureHpdsLib\n",
    "import PicSureClient\n",
    "\n",
    "# python_lib for pic-sure\n",
    "from python_lib.utils import get_multiIndex_variablesDict\n",
    "\n",
    "# analysis\n",
    "import pandas as pd\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print metadata\n",
    "print(\"The PIC-SURE API libraries versions you've been downloading are: \\n- PicSureClient: {0}\\n- PicSureHpdsLib: {1}\".format(PicSureClient.__version__, PicSureHpdsLib.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"UDN database time stamp: {}\".format(then))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to a PIC-SURE network\n",
    "\n",
    "### 1. Connect to the GIC institute data network using the HPDS adapter\n",
    "Several information are needed to get access to data through the PIC-SURE API: a network URL, a resource id, and a user security token which is specific to a given URL + resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token is the individual user key given to connect to the GIC institute resource\n",
    "token_file = \"token.txt\"\n",
    "with open(token_file, \"r\") as f:\n",
    "    my_token = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get connection object\n",
    "connection = PicSureClient.Client.connect(url = PICSURE_network_URL,\n",
    "                                 token = my_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get adapter object\n",
    "adapter = PicSureHpdsLib.Adapter(connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connection to the PIC-SURE API w/ key\n",
    "# network information\n",
    "PICSURE_network_URL = \"https://udn.hms.harvard.edu/picsure\"\n",
    "resource_id = \"c23b6814-7e5b-48d2-80d9-65511d7d2051\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get resource object\n",
    "resource = adapter.useResource(resource_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three objects are created here: a connection, an adapter and a resource object, using respectively the `picsure` and `hpds` libraries.\n",
    "\n",
    "As we will only be using one single resource, **the resource object is actually the only one we will need to proceed with data analysis hereafter** (FYI, the connection object is useful to get access to different databases stored in different resources).\n",
    "\n",
    "It is connected to the specific data source ID we specified, and enables to query and retrieve data from this source.\n",
    "\n",
    "#### Getting help with the Python PIC-SURE API\n",
    "\n",
    "Each object exposed by the PicSureHpdsLib library got a help() method. Calling it will print out a helper message about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get resource documentation\n",
    "resource.help()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, this output tells us that this resource object got 2 methods, and it gives insights about their function.\n",
    "\n",
    "### 2. Explore the data: data structures description\n",
    "\n",
    "There are two methods to explore the data from which the user get two different data structures: a **dictionary object** to explore variables and a **query object** to explore the patient records in UDN. \n",
    "\n",
    "**Methods**:\n",
    "\n",
    "    * Search variables: find() method\n",
    "    * Retrieve data: query() methods\n",
    "\n",
    "**Data structures**:\n",
    "\n",
    "    * Dictionary object structure\n",
    "    * Query object structure\n",
    "    \n",
    "\n",
    "#### Explore variables using the _dictionary_\n",
    "\n",
    "Once a connection to the desired resource has been established, we first need to get a quick idea of which variables are available in the database. To this end, we will use the `dictionary` method of the `resource` object.\n",
    "\n",
    "A dictionary object offers the possibility to retrieve information about either matching variables according to a specific term or all available variables, using the `find()` method. For instance, looking for variables containing the term 'aplasia' is done this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary object and search for a specific term, in this example for \"aplasia\"\n",
    "dictionary = resource.dictionary()\n",
    "lookup = dictionary.find(\"aplasia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have created the dictionary object with only variables matched by the search term. To retrieve the search result from dictionary objects we have 4 different methods: `count()`, `keys()`, `entries()`, and `DataFrame()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# description of the dictionary search content\n",
    "pprint({\"Count\": lookup.count(), \n",
    "        \"Keys\": lookup.keys()[0:2],\n",
    "        \"Entries\": lookup.entries()[0:2]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DataFrame()** enables to get the result of the dictionary search in a pandas dataframe format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show table of records from the dictionary object\n",
    "lookup.DataFrame().tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can retrieve information about **ALL** variables. We do it without specifying a term in the dictionary search method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we search the whole set of variables\n",
    "plain_variablesDict = resource.dictionary().find().DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# description of the whole dictionary of variables\n",
    "print(plain_variablesDict.shape)\n",
    "plain_variablesDict.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Variant variables\n",
    "plain_variablesDict[plain_variablesDict.HpdsDataType == 'info']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The UDN network resource contains 13414 variables described by 10 data fields:\n",
    "* HpdsDataType\n",
    "* description\n",
    "* categorical\n",
    "* categoryValues\n",
    "* values\n",
    "* continuous\n",
    "* min\n",
    "* max\n",
    "* observationCount\n",
    "* patientCount\n",
    "\n",
    "The dictionary provide various information about the variables, such as:\n",
    "\n",
    "* observationCount: number of entries with non-null value\n",
    "* categorical: type of the variables, True if categorical, False if continuous/numerical\n",
    "* min/max: only provided for non-categorical variables\n",
    "* HpdsDataType: 'phenotypes' or 'genotypes'. Currently, the API only expsoses'phenotypes' variables\n",
    "\n",
    "Hence, it enables to:\n",
    "\n",
    "* Use the various variables information as criteria for variable selection.\n",
    "* Use the row names of the DataFrame to get the actual variables names, to be used in the query, as shown below.\n",
    " \n",
    "Variable names (`KEY` or row **indexes** in the dataframe), as currently implemented in the API, aren't straightforward to use because:\n",
    "\n",
    "1. Very long\n",
    "2. Presence of backslashes that requires modification right after copy-pasting.\n",
    "\n",
    "However, using the dictionary to select variables can help to deal with this. \n",
    "\n",
    "##### Parsing variable names\n",
    "We can use an utils function, `get_multiIndex_variablesDict()`, defined in python_lib/utils.py, to add a little more information and ease working with variables names. It takes advantage of pandas MultiIndex functionality see [pandas official documentation on this topic](https://pandas.pydata.org/pandas-docs/stable/user_guide/advanced.html).\n",
    "\n",
    "Although not an official feature of the API, such functionality illustrates how to quickly scan and select groups of related variables.\n",
    "\n",
    "Printing the \"MultiIndex\" Dictionary allows to quickly see the tree-like organisation of the variables. Moreover, original and simplified variable names are now stored respectively in the \"varName\" and \"simplified_varName\" columns (simplified variable names is simply the last component of the variable name, which usually makes the most sense to know what each variable is about)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the variables tree hierarchy from the variables name\n",
    "variablesDict = get_multiIndex_variablesDict(plain_variablesDict)\n",
    "variablesDict.loc[[\"04_Clinical symptoms and physical findings (in HPO, from PhenoTips)\"], :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a simple example to illustrate the ease of use of a MultiIndex dictionary. Let's say we are interested in filtering variables related to \"aplasias\" in the \"nervous system\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_system = variablesDict.index.get_level_values(2) == \"Abnormality of the nervous system\"\n",
    "mask_abnormality = variablesDict.varName.str.contains('Aplasia')\n",
    "filtered_variables = variablesDict.loc[mask_system & mask_abnormality,]\n",
    "print(filtered_variables.shape)\n",
    "filtered_variables.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although pretty simple, it can be easily combined with other filters to quickly select necessary variables.\n",
    "\n",
    "#### Explore patient records using _query_\n",
    "\n",
    "Beside from the dictionary, the second cornerstone of the API are the query methods (`select()`, `require()`, `anyof()`, `filter()`). They are the entering point to **query and retrieve data from the resource**.\n",
    "\n",
    "First, we need to create a query object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a query object for the resource\n",
    "my_query = resource.query()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The query object created will be then passed to the different query methods to build the query: <font color='orange'>select().add(), require().add(), anyof().add(), and filter().add()</font>.\n",
    "\n",
    "* The **select().add()** method accept variable names as string or list of strings as argument, and will allow the query to return all variables included in the list, without any record (ie subjects/rows) subsetting.\n",
    "* The **require().add()** method accept variable names as string or list of strings as argument, and will allow the query to return all the variables passed, and only records that do not contain any null values for those variables.\n",
    "* The **anyof().add()** method accept variable names as string or list of strings as argument, and will allow the query to return all variables included in the list, and only records that do contain at least one non-null value for those variables.\n",
    "* The **filter().add()** method accept variable names a variable name as strings as argument, plus additional values to filter on that given variable. The query will return this variable and only the records that do match this filter.\n",
    "\n",
    "All those 4 methods can be combined when building a query. The record eventually returned by the query have to meet all the different specified filters.\n",
    "\n",
    "##### Building the query\n",
    "Let's say we want to check some demographics about the data in UDN. We will filter to variables that have observation counts > 50% patient counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select demographic variable names\n",
    "demographicsDict = resource.dictionary().find(\"demographics\")\n",
    "mask_obs = demographicsDict.DataFrame().observationCount > demographicsDict.DataFrame().patientCount * .50\n",
    "selected_varnames = demographicsDict.DataFrame()[mask_obs].index.to_list()\n",
    "print(len(selected_varnames))\n",
    "selected_varnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build and query for demographics patient data\n",
    "my_query.select().add(selected_varnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Retrieving the data\n",
    "Once our query object is finally built, we use the `getResultsDataFrame()` method to retrieve the data corresponding to our query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# retrieve the query result as a dataframe\n",
    "demographics_data = my_query.getResultsDataFrame(low_memory=False).set_index(\"Patient ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(demographics_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographics_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have retieved patient records in UDN that meet the criteria posed in the query. \n",
    "\n",
    "**NOTE**: The <font color='orange'>Patient ID</font> is the `KEY` or row `INDEX` of the dataframe derived.\n",
    "\n",
    "From this point, we can proceed with the data management and analysis using any other Python functions or libraries.\n",
    "\n",
    "##### Visualize the demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# rename column names\n",
    "demographics_data = demographics_data.rename(columns={\"\\\\00_Demographics\\Age at UDN Evaluation (in years)\\\\\": \"age_udn\",\n",
    "                                  \"\\\\00_Demographics\\Age at symptom onset in years\\\\\": \"age_symptom\",\n",
    "                                  \"\\\\00_Demographics\\Current age in years\\\\\": \"age_current\",\n",
    "                                  \"\\\\00_Demographics\\Ethnicity\\\\\": \"ethnicity\",\n",
    "                                  \"\\\\00_Demographics\\Sex\\\\\": \"sex\",\n",
    "                                  \"\\\\00_Demographics\\Race\\\\\": \"race\"\n",
    "                                 })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize \n",
    "demographics_data.groupby(['race']).size().plot.pie( figsize=(10, 5), title=\"Race distribution in UDN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Variant Explorer\n",
    "The API provides functionality to query on variant fields, and query for variant data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example querying for categorical variant data. getVariantsCount retrieves an approximate variant count.\n",
    "\n",
    "my_query = resource.query()\n",
    "my_query.filter().add(\"Gene_with_variant\", \"CHD8\")\n",
    "my_query.getCount()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before calling getVariantsDataFrame, ensure that getVariantsCount is a reasonable size. If not, refine your query further.\n",
    "Queries returning more than 100,000 variants could crash your workbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_query.getVariantsDataFrame(low_memory=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example of a genomic filter is looking at the variant frequency.\n",
    "- Novel variants are not found in the rest of the population\n",
    "- Rare variants are found in <1% of the population\n",
    "- Common variants are found in >= 1% of the population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filtering by Gene prior to adding additional genomic or phenotypic filters is good practice to ensure the system does not become overwhelmed by a very large query.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example querying for rare variants in the following genes of interest: CHD8, CHD9, and CHCHD10\n",
    "\n",
    "genes_of_interest = [\"CHD8\", \"CHD9\", \"CHCHD10\"]\n",
    "my_query = resource.query()\n",
    "my_query.filter().add(\"Gene_with_variant\", genes_of_interest)\n",
    "my_query.filter().add(\"Variant_frequency_as_text\", \"Rare\")\n",
    "my_query.getVariantsDataFrame(low_memory=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can further add a phenotypic filter to this existing genomic query, to find rare variants in the genes of interest, where the sex of the participant is \"Female\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example combining variant and phenotype queries\n",
    "\n",
    "my_query.filter().add(\"\\\\00_Demographics\\\\Sex\\\\\", \"Female\")\n",
    "my_query.getVariantsDataFrame(low_memory=False)\n",
    "#my_query.getResultsDataFrame(low_memory=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Patient ID Mapping\n",
    "You may notice that the Patient IDs found in the demographics dataframe do not match the Patient IDs found from our genomic query. Phenotypic queries return 'Patient IDs' while genomic queries return 'UDN IDs'. You can create a mapping between these two types of IDs as demonstrated below, which you can use to merge phenotypic and genomic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_query = resource.query()\n",
    "mapping_query.select().add(\"\\\\000_UDN ID\\\\\")\n",
    "mapping_query.getResultsDataFrame(low_memory=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
